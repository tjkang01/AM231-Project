{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in STAT 110 for now; convert to lower case\n",
    "filename = \"cleaned_data/STAT/STAT 110.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  349982\n",
      "Total Vocab:  66\n",
      "Total Patterns:  349882\n"
     ]
    }
   ],
   "source": [
    "# Map unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# Prepare the dataset of input to output pairs encoded as integers\n",
    "# TODO: try (padded) sentences instead of sequences of length 100?\n",
    "# see textblob and https://keras.io/preprocessing/sequence/\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# Reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# Normalize\n",
    "X = X / float(n_vocab)\n",
    "# One hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Define the checkpoint\n",
    "filepath=\"keras_checkpoints/weights-improvement-{epoch:02d}-{loss:.4f}-stat110-chars.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "349882/349882 [==============================] - 1743s 5ms/step - loss: 2.8459\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.84595, saving model to keras_checkpoints/weights-improvement-01-2.8459.hdf5\n",
      "Epoch 2/20\n",
      "349882/349882 [==============================] - 1760s 5ms/step - loss: 2.6077\n",
      "\n",
      "Epoch 00002: loss improved from 2.84595 to 2.60772, saving model to keras_checkpoints/weights-improvement-02-2.6077.hdf5\n",
      "Epoch 3/20\n",
      "349882/349882 [==============================] - 1734s 5ms/step - loss: 2.3955\n",
      "\n",
      "Epoch 00003: loss improved from 2.60772 to 2.39548, saving model to keras_checkpoints/weights-improvement-03-2.3955.hdf5\n",
      "Epoch 4/20\n",
      "349882/349882 [==============================] - 1740s 5ms/step - loss: 2.2513\n",
      "\n",
      "Epoch 00004: loss improved from 2.39548 to 2.25133, saving model to keras_checkpoints/weights-improvement-04-2.2513.hdf5\n",
      "Epoch 5/20\n",
      "349882/349882 [==============================] - 1550s 4ms/step - loss: 2.1472\n",
      "\n",
      "Epoch 00005: loss improved from 2.25133 to 2.14723, saving model to keras_checkpoints/weights-improvement-05-2.1472.hdf5\n",
      "Epoch 6/20\n",
      "349882/349882 [==============================] - 1426s 4ms/step - loss: 2.0689\n",
      "\n",
      "Epoch 00006: loss improved from 2.14723 to 2.06885, saving model to keras_checkpoints/weights-improvement-06-2.0689.hdf5\n",
      "Epoch 7/20\n",
      "349882/349882 [==============================] - 26888s 77ms/step - loss: 2.0085\n",
      "\n",
      "Epoch 00007: loss improved from 2.06885 to 2.00851, saving model to keras_checkpoints/weights-improvement-07-2.0085.hdf5\n",
      "Epoch 8/20\n",
      "349882/349882 [==============================] - 4710s 13ms/step - loss: 1.9574\n",
      "\n",
      "Epoch 00008: loss improved from 2.00851 to 1.95739, saving model to keras_checkpoints/weights-improvement-08-1.9574.hdf5\n",
      "Epoch 9/20\n",
      "349882/349882 [==============================] - 1877s 5ms/step - loss: 1.9142\n",
      "\n",
      "Epoch 00009: loss improved from 1.95739 to 1.91416, saving model to keras_checkpoints/weights-improvement-09-1.9142.hdf5\n",
      "Epoch 10/20\n",
      "349882/349882 [==============================] - 2501s 7ms/step - loss: 1.8754\n",
      "\n",
      "Epoch 00010: loss improved from 1.91416 to 1.87541, saving model to keras_checkpoints/weights-improvement-10-1.8754.hdf5\n",
      "Epoch 11/20\n",
      "349882/349882 [==============================] - 1867s 5ms/step - loss: 1.8434\n",
      "\n",
      "Epoch 00011: loss improved from 1.87541 to 1.84336, saving model to keras_checkpoints/weights-improvement-11-1.8434.hdf5\n",
      "Epoch 12/20\n",
      "349882/349882 [==============================] - 5853s 17ms/step - loss: 1.8133\n",
      "\n",
      "Epoch 00012: loss improved from 1.84336 to 1.81326, saving model to keras_checkpoints/weights-improvement-12-1.8133.hdf5\n",
      "Epoch 13/20\n",
      "349882/349882 [==============================] - 1798s 5ms/step - loss: 1.7876\n",
      "\n",
      "Epoch 00013: loss improved from 1.81326 to 1.78763, saving model to keras_checkpoints/weights-improvement-13-1.7876.hdf5\n",
      "Epoch 14/20\n",
      "349882/349882 [==============================] - 4175s 12ms/step - loss: 1.7650\n",
      "\n",
      "Epoch 00014: loss improved from 1.78763 to 1.76495, saving model to keras_checkpoints/weights-improvement-14-1.7650.hdf5\n",
      "Epoch 15/20\n",
      "349882/349882 [==============================] - 2437s 7ms/step - loss: 1.7410\n",
      "\n",
      "Epoch 00015: loss improved from 1.76495 to 1.74101, saving model to keras_checkpoints/weights-improvement-15-1.7410.hdf5\n",
      "Epoch 16/20\n",
      "349882/349882 [==============================] - 2904s 8ms/step - loss: 1.7229\n",
      "\n",
      "Epoch 00016: loss improved from 1.74101 to 1.72286, saving model to keras_checkpoints/weights-improvement-16-1.7229.hdf5\n",
      "Epoch 17/20\n",
      "349882/349882 [==============================] - 1597s 5ms/step - loss: 1.7058\n",
      "\n",
      "Epoch 00017: loss improved from 1.72286 to 1.70580, saving model to keras_checkpoints/weights-improvement-17-1.7058.hdf5\n",
      "Epoch 18/20\n",
      "349882/349882 [==============================] - 2031s 6ms/step - loss: 1.6905\n",
      "\n",
      "Epoch 00018: loss improved from 1.70580 to 1.69048, saving model to keras_checkpoints/weights-improvement-18-1.6905.hdf5\n",
      "Epoch 19/20\n",
      "349882/349882 [==============================] - 1453s 4ms/step - loss: 1.6747\n",
      "\n",
      "Epoch 00019: loss improved from 1.69048 to 1.67467, saving model to keras_checkpoints/weights-improvement-19-1.6747.hdf5\n",
      "Epoch 20/20\n",
      "349882/349882 [==============================] - 1450s 4ms/step - loss: 1.6601\n",
      "\n",
      "Epoch 00020: loss improved from 1.67467 to 1.66006, saving model to keras_checkpoints/weights-improvement-20-1.6601.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18268a2f4e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" just make sure you know the cost.\n",
      "great class to get an introduction to probability theory. problem  \"\n",
      "sets are axasome.  the class is very hirprable to seee the course tiet you den a lot of time and tfe coas and toeerstanding the mrte taan toelrs and toeerstanding the mrte taan toalrstand the material and the class is a great ceasgrgun class that would be iese you ane toeerstanding the material and the class is a great ceasgrgun class that would be iese you ane a lot of time to toeerstand the material. toet 110 is a great class that iave to ae anllitenl metel and the class is a great ceasgrgun class that would be iese you ane toeerstanding the material and the class is a great ceasgrgun class that would be iese you ane a lot of time to toeerstand the material. toet 110 is a great class that iave to ae anllitenl metel and the class is a great ceasgrgun class that would be iese you ane toeerstanding the material and the class is a great ceasgrgun class that would be iese you ane a lot of time to toeerstand the material. toet 110 is a great class that iave to ae anllitenl metel and the cl\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"keras_checkpoints/weights-improvement-20-1.6601.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a bigger model\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(256))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(y.shape[1], activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "filepath=\"keras_checkpoints/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "349882/349882 [==============================] - 4968s 14ms/step - loss: 2.6992\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.69919, saving model to weights-improvement-01-2.6992-bigger.hdf5\n",
      "Epoch 2/20\n",
      "349882/349882 [==============================] - 4715s 13ms/step - loss: 2.1683\n",
      "\n",
      "Epoch 00002: loss improved from 2.69919 to 2.16834, saving model to weights-improvement-02-2.1683-bigger.hdf5\n",
      "Epoch 3/20\n",
      "349882/349882 [==============================] - 4741s 14ms/step - loss: 1.9108\n",
      "\n",
      "Epoch 00003: loss improved from 2.16834 to 1.91078, saving model to weights-improvement-03-1.9108-bigger.hdf5\n",
      "Epoch 4/20\n",
      "349882/349882 [==============================] - 4721s 13ms/step - loss: 1.7746\n",
      "\n",
      "Epoch 00004: loss improved from 1.91078 to 1.77462, saving model to weights-improvement-04-1.7746-bigger.hdf5\n",
      "Epoch 5/20\n",
      "349882/349882 [==============================] - 4713s 13ms/step - loss: 1.6846\n",
      "\n",
      "Epoch 00005: loss improved from 1.77462 to 1.68462, saving model to weights-improvement-05-1.6846-bigger.hdf5\n",
      "Epoch 6/20\n",
      "349882/349882 [==============================] - 4709s 13ms/step - loss: 1.6217\n",
      "\n",
      "Epoch 00006: loss improved from 1.68462 to 1.62170, saving model to weights-improvement-06-1.6217-bigger.hdf5\n",
      "Epoch 7/20\n",
      "349882/349882 [==============================] - 9435s 27ms/step - loss: 1.5722\n",
      "\n",
      "Epoch 00007: loss improved from 1.62170 to 1.57220, saving model to weights-improvement-07-1.5722-bigger.hdf5\n",
      "Epoch 8/20\n",
      "349882/349882 [==============================] - 8059s 23ms/step - loss: 1.5313\n",
      "\n",
      "Epoch 00008: loss improved from 1.57220 to 1.53127, saving model to weights-improvement-08-1.5313-bigger.hdf5\n",
      "Epoch 9/20\n",
      "349882/349882 [==============================] - 9178s 26ms/step - loss: 1.4990\n",
      "\n",
      "Epoch 00009: loss improved from 1.53127 to 1.49898, saving model to weights-improvement-09-1.4990-bigger.hdf5\n",
      "Epoch 10/20\n",
      "349882/349882 [==============================] - 9292s 27ms/step - loss: 1.4691\n",
      "\n",
      "Epoch 00010: loss improved from 1.49898 to 1.46915, saving model to weights-improvement-10-1.4691-bigger.hdf5\n",
      "Epoch 11/20\n",
      "349882/349882 [==============================] - 4807s 14ms/step - loss: 1.4454\n",
      "\n",
      "Epoch 00011: loss improved from 1.46915 to 1.44540, saving model to weights-improvement-11-1.4454-bigger.hdf5\n",
      "Epoch 12/20\n",
      "349882/349882 [==============================] - 4697s 13ms/step - loss: 1.4228\n",
      "\n",
      "Epoch 00012: loss improved from 1.44540 to 1.42276, saving model to weights-improvement-12-1.4228-bigger.hdf5\n",
      "Epoch 13/20\n",
      "349882/349882 [==============================] - 4692s 13ms/step - loss: 1.4027\n",
      "\n",
      "Epoch 00013: loss improved from 1.42276 to 1.40272, saving model to weights-improvement-13-1.4027-bigger.hdf5\n",
      "Epoch 14/20\n",
      "349882/349882 [==============================] - 4815s 14ms/step - loss: 1.3874\n",
      "\n",
      "Epoch 00014: loss improved from 1.40272 to 1.38743, saving model to weights-improvement-14-1.3874-bigger.hdf5\n",
      "Epoch 15/20\n",
      "349882/349882 [==============================] - 4729s 14ms/step - loss: 1.3698\n",
      "\n",
      "Epoch 00015: loss improved from 1.38743 to 1.36983, saving model to weights-improvement-15-1.3698-bigger.hdf5\n",
      "Epoch 16/20\n",
      "349882/349882 [==============================] - 4856s 14ms/step - loss: 1.3547\n",
      "\n",
      "Epoch 00016: loss improved from 1.36983 to 1.35465, saving model to weights-improvement-16-1.3547-bigger.hdf5\n",
      "Epoch 17/20\n",
      "349882/349882 [==============================] - 4715s 13ms/step - loss: 1.3418\n",
      "\n",
      "Epoch 00017: loss improved from 1.35465 to 1.34176, saving model to weights-improvement-17-1.3418-bigger.hdf5\n",
      "Epoch 18/20\n",
      "349882/349882 [==============================] - 4734s 14ms/step - loss: 1.3300\n",
      "\n",
      "Epoch 00018: loss improved from 1.34176 to 1.32999, saving model to weights-improvement-18-1.3300-bigger.hdf5\n",
      "Epoch 19/20\n",
      "349882/349882 [==============================] - 4720s 13ms/step - loss: 1.3202\n",
      "\n",
      "Epoch 00019: loss improved from 1.32999 to 1.32021, saving model to weights-improvement-19-1.3202-bigger.hdf5\n",
      "Epoch 20/20\n",
      "349882/349882 [==============================] - 4716s 13ms/step - loss: 1.3097\n",
      "\n",
      "Epoch 00020: loss improved from 1.32021 to 1.30967, saving model to weights-improvement-20-1.3097-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x182044d0278>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit\n",
    "model2.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" fe things, such as how often a bus stops at a bus stop. be prepared to study for it, but it's really \"\n",
      " hard and the course is very difficult. but it is a great class that i dould have taken at harvard. the course is very difficult, but it is a great class that i dould have taken at harvard. the course is very difficult, but it is a great class that i dould have taken at harvard. the course is very difficult, but it is a great class that i dould have taken at harvard. the course is very difficult, but it is a great class that i dould have taken at harvard. the course is very difficult, but it is a great class that i dould have taken at harvard. the course is very difficult, but it is a great class that i dould have taken at harvard. the course is very difficult, but it is a great class that i dould have taken at harvard. the course is very difficult, but it is a great class that i dould have taken at harvard. the course is very difficult, but it is a great class that i dould have taken at harvard. the course is very difficult, but it is a great class that i dould have taken at harvard. \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"keras_checkpoints/weights-improvement-19-1.3202-bigger.hdf5\"\n",
    "model2.load_weights(filename)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model2.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
